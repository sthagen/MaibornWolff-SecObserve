{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"development/overview/","title":"Overview","text":""},{"location":"development/overview/#backend","title":"Backend","text":""},{"location":"development/overview/#frontend","title":"Frontend","text":""},{"location":"getting_started/about/","title":"About SecObserve","text":"<p>SecObserve in a nutshell</p> <p>SecObserve is an open source vulnerability management system for software development teams that supports a variety of open source vulnerability scanners and integrates easily into CI/CD pipelines.</p> <p></p>"},{"location":"getting_started/about/#overview","title":"Overview","text":"<p>The aim of SecObserve is to make vulnerability scanning and vulnerability management as easy as possible for software development projects using open source tools. It consists of 2 major components:</p> <ul> <li> <p>Vulnerability management system SecObserve: SecObserve provides the development team with an overview of the results of all vulnerability scans for their project, which can be easily filtered and sorted. In the detailed view, the results are displayed uniformly with a wealth of information, regardless of which vulnerability scanner generated them.</p> <p>With the help of automatically executed rules and manual assessments, the results can be efficiently evaluated to eliminate irrelevant results and accept risks. This allows the development team to concentrate on fixing the relevant vulnerabilities.</p> </li> <li> <p>GitLab CI templates and GitHub actions: Integrating vulnerability scanners into a CI/CD pipeline can be tedious. Each tool has to be installed differently and is called with different parameters. To avoid having to solve this task all over again, there are repositories with GitLab CI Templates and GitHub Actions. These make the process of integrating vulnerability scanners very simple by providing uniform methods for launching the tools and uniform parameters. The tools are regularly updated in the repositories so that the latest features and bug fixes are always available.</p> <p>All actions and templates run the scanner, upload the results into SecObserve and make the results of the scans available for download as artefacts in JSON format.</p> <p>The sources of the GitHub actions and GitLab CI templates can be found in https://github.com/MaibornWolff/secobserve_actions_templates.</p> </li> </ul> <p></p>"},{"location":"getting_started/about/#procedure","title":"Procedure","text":"<p>A sequence diagram explains the basic procedure how to work with SecObserve:</p> <pre><code>sequenceDiagram\n    autonumber\n    actor Developer\n    Developer -&gt;&gt; Repository: Check in source\n    activate Repository\n    Repository -&gt;&gt; Pipeline: Start pipeline\n    deactivate Repository\n    activate Pipeline\n    Pipeline -&gt;&gt; Pipeline: Run scanners\n    Pipeline -&gt;&gt; SecObserve: Upload results\n    deactivate Pipeline\n    activate SecObserve\n    SecObserve -&gt;&gt; SecObserve: Apply rules\n    deactivate SecObserve\n    Developer -&gt;&gt; SecObserve: View observations\n    Developer -&gt;&gt; SecObserve: Assess observations\n    Developer -&gt;&gt; Developer: Implement fixes\n    Developer -&gt;&gt; Repository: Check in source ...</code></pre> <ol> <li>A developer implements a feature and checks in his code to the repository</li> <li>The repository starts a pipeline for the change</li> <li>The pipeline runs several of the supported vulnerability scanners. To make integration easy, SecObserve provides predefined templates for the most relevant scanners, see GitHub actions and GitLab CI templates.</li> <li>The scanners store their results in files, which are uploaded into SecObserve.</li> <li>SecObserve applies rules to adjust severity and status of observations during the upload process.</li> <li>The developer can now look at the observations in SecObserve, to see what has changed ...</li> <li>... and if necessary assess observations to change their status (eg. false positive or risk accepted) or severity.</li> <li>If fixes are needed to close vulnerabilities, the developer will implement the fixes ...</li> <li>... and check them in to the repository. Now the cycle starts again.</li> </ol>"},{"location":"getting_started/anatomy_of_an_observation/","title":"Anatomy of an observation","text":""},{"location":"getting_started/anatomy_of_an_observation/#observation","title":"Observation","text":"<ul> <li>The Severity can have 3 sources: <ul> <li>Initially the parser sets a severity, based on the incoming data. </li> <li>If there is a rule configured that matches the observation, it overrides the severity set by the parser.</li> <li>When a user assesses the observation and sets a different severity, this severity from the assessment overrides the severity set by a rule and the severity set by the parser.</li> </ul> </li> <li>The initial Status set by an import is <code>Open</code>. It will be set to <code>Resolved</code> if the same observation is not found in a subsequent import. On the other hand, resolved observations are set back to the status <code>Open</code> if they reappear in a later import. As for the severity, if a rule matches the observation or a user sets a different status in an assessment, these changes will override the status set by the import.</li> <li>Title and Description are short and long explanations what the observation is about.</li> <li>Scanners might suggest a Recommendation (not shown in the screenshot).</li> </ul>"},{"location":"getting_started/anatomy_of_an_observation/#vulnerability","title":"Vulnerability","text":"<p>(not shown in the screenshot)</p> <p>Vulnerability data can comprise a Vulnerability Id like a CVE or GHSA, a CVSSv3 score and CVSSv3 vector as well as a CWE number.</p>"},{"location":"getting_started/anatomy_of_an_observation/#origins","title":"Origins","text":"<p>An observation can be found at different origins:</p> <ul> <li>Service: A service is a self-contained piece of functionality within a product. This can be something like frontend or backend or the name of a microservice.</li> <li>Component: Typically a library (Maven, NPM, PyPI, ...) or a program installed in a docker image, identified by name and version.</li> <li>Docker image: Name and tag of a Docker image, where the observation was found.</li> <li>Endpoint: The URL of a web address.</li> <li>Source file: Path and name of a source file, start and end lines are optional. The source file will be shown as a link to the source in the repository, if a Repository prefix has been configured in the product.</li> <li>Cloud: The name of the cloud provider, account (AWS) or subscription (Azure) or project (GCP), resource and resource type, where the observation was found.</li> </ul>"},{"location":"getting_started/anatomy_of_an_observation/#log","title":"Log","text":"<p>Every time either the severity or the status get changed by an import or an assessment, this event is recorded in the Observation Log together with a comment.</p>"},{"location":"getting_started/anatomy_of_an_observation/#potential-duplicates","title":"Potential duplicates","text":"<p>(not shown in the screenshot)</p> <p>If an observation has potential duplicates, they are listed here and can be marked as duplicates.</p>"},{"location":"getting_started/anatomy_of_an_observation/#metadata","title":"Metadata","text":"<p>Some information about how the observation was created or updated.</p>"},{"location":"getting_started/anatomy_of_an_observation/#references","title":"References","text":"<p><code>References</code> are links to further information about the observation. They are imported with the observation.</p>"},{"location":"getting_started/anatomy_of_an_observation/#evidences","title":"Evidences","text":"<p><code>Evidences</code> are extracts from the scan reports showing the basis on which the observation was created.</p>"},{"location":"getting_started/architecture/","title":"Architecture","text":""},{"location":"getting_started/architecture/#frontend","title":"Frontend","text":"<p>The frontend is a single page application (SPA), implemented with TypeScript, React and the React-Admin framework. The page is delivered by a nginx server.</p>"},{"location":"getting_started/architecture/#backend","title":"Backend","text":"<p>The backend is implemented with Python and Django / Django Rest Framework. A Gunicorn server delivers 2 major components:</p>"},{"location":"getting_started/architecture/#rest-api","title":"REST API","text":"<p>The REST API is used by the frontend to serve and manipulate data and by CI/CD pipelines to upload scan results.</p>"},{"location":"getting_started/architecture/#django-admin","title":"Django Admin","text":"<p>The Django Admin interface is used by administrators to manage users and some system-wide configurations.</p>"},{"location":"getting_started/architecture/#database","title":"Database","text":"<p>Currently MySQL and PostgreSQL are supported as databases.</p>"},{"location":"getting_started/configuration/","title":"Configuration","text":""},{"location":"getting_started/configuration/#deployment","title":"Deployment","text":"<p>A part of the configuation is done with environment variables, which need to be set when deploying SecObserve. How this is done depends on the deployment method, see Installation.</p>"},{"location":"getting_started/configuration/#backend","title":"Backend","text":"Environment variable Optionality Description <code>ADMIN_USER</code> mandatory Username of the administration user. The user will be created at the fist start of the backend. <code>ADMIN_EMAIL</code> optional E-Mail of the administration user. <code>ADMIN_PASSWORD</code> optional Initial password of the admin user. If it is not set, a random password will be created during startup and shown in the log. <code>ALLOWED_HOSTS</code> mandatory Hostname of the backend, see Django settings ALLOWED_HOSTS <code>CORS_ALLOWED_ORIGINS</code> mandatory URL of the frontend that is authorized to make cross-site HTTP requests. <code>DATABASE_HOST</code> mandatory Which host to use when connecting to the database. <code>DATABASE_DB</code> mandatory The name of the database to use. <code>DATABASE_PORT</code> mandatory The port to use when connecting to the database. <code>DATABASE_USER</code> mandatory The username to use when connecting to the database. <code>DATABASE_PASSWORD</code> mandatory The password to use when connecting to the database. <code>DATABASE_ENGINE</code> mandatory The database backend to use. Supported database backends are <code>django.db.backends.mysql</code> and <code>django.db.backends.postgresql</code> <code>MYSQL_AZURE</code> optional Must be set if Azure Database for MySQL is used, to use the necessary SSL certificate. For MySQL Flexible Server it needs to have the value <code>flexible</code>, for MySQL Single Server the the value needs to be <code>single</code>. See Connect using mysql command-line client with TLS/SSL and Configure SSL connectivity in your application to securely connect to Azure Database for MySQL. <code>DJANGO_SECRET_KEY</code> mandatory A secret key for a particular Django installation. This is used to provide cryptographic signing, and should be set to a unique, unpredictable value with at least 50 characters, see Django settings SECRET_KEY. <code>FIELD_ENCRYPTION_KEY</code> mandatory Key to encrypt fields like the JWT secret. See Generating an Encryption Key how to generate the key. <code>GUNICORN_WORKERS</code> optional Number of worker processes for the Gunicorn web server, see Gunicorn documentation. Default is 3. <code>GUNICORN_THREADS</code> optional Number of worker threads for the Gunicorn web server, default is 10. <code>OIDC_AUTHORITY</code> mandatory The authority is a URL that hosts the OpenID configuration well-known endpoint. <code>OIDC_CLIENT_ID</code> mandatory The client ID is the unique Application (client) ID assigned to your app by the OpenID Connect provider when the app was registered. <code>OIDC_USERNAME</code> mandatory The claim that contains the username to find or create the user. <code>OIDC_FIRST_NAME</code> mandatory The claim that contains the first name of the user. <code>OIDC_LAST_NAME</code> mandatory The claim that contains the last name of the user. <code>OIDC_FULL_NAME</code> mandatory The claim that contains the full name of the user. <code>OIDC_EMAIL</code> mandatory The claim that contains the email address of the user."},{"location":"getting_started/configuration/#frontend","title":"Frontend","text":"Environment variable Optionality Description <code>API_BASE_URL</code> mandatory URL where to find the backend API, e.g. <code>https:\\\\secobserve-backend.example.com/api</code>. <code>OIDC_ENABLE</code> mandatory <code>true</code>: OpenID Connect authentication is active, <code>false</code>: otherwise. <code>OIDC_AUTHORITY</code> mandatory The authority is a URL that hosts the OpenID Connect configuration well-known endpoint. <code>OIDC_CLIENT_ID</code> mandatory The client ID is the unique Application (client) ID assigned to your app by the OpenID Connect provider when the app was registered. <code>OIDC_REDIRECT_URI</code> mandatory The redirect URI is the URI the identity provider will send the security tokens back to. To be set with the URL of the frontend. <code>OIDC_POST_LOGOUT_REDIRECT_URI</code> mandatory The post logout redirect URI is the URI that will be called after logout. To be set with the URL of the frontend. <p>All the <code>OIDC_*</code> environment variables are needed for technical reasons. If <code>OIDC_ENABLE</code> is set to <code>false</code>, the other <code>OIDC_*</code> environment variables can be set to <code>dummy</code> or something similar.</p> <p>More about the configuration for different OpenID Connect providers can be found in OpenID Connect authentication.</p>"},{"location":"getting_started/configuration/#admin-user-interface","title":"Admin user interface","text":"<p>SecObserve provides an administration user interface to manage users and some system-wide configurations. It can be accessed via <code>&lt;BACKEND_URL&gt;/admin</code> by users where the flag <code>Superuser status</code> is set. </p> <p>The entries in section CONSTANCE / Config should be checked and adjusted if necessary after installing SecObserve.</p>"},{"location":"getting_started/data_model/","title":"Data model","text":"<pre><code>erDiagram\n    Product_Group |o--o{ Product : has\n    Product ||--o{ Observation : has\n    Product ||--o{ Branch : has\n    Product ||--o{ Service : has\n    Product ||--o{ Vulnerability_Check : has\n    Product ||--o{ Product_Rule : has\n    Product ||--o{ API_Configuration : has\n    Product ||--o{ Product_Member : has\n    Parser ||--o{ Observation: discovered_by\n    Observation }o--o| Branch: found_in\n    Observation ||--|{ Observation_Log : has\n    Observation ||--|{ Reference : has\n    Observation ||--|{ Evidence : has\n    General_Rule</code></pre>"},{"location":"getting_started/data_model/#product-group","title":"Product Group","text":"<p>A <code>Product Group</code> is a collection of products. It is used to group products that belong together, e.g. because they are part of the same project. In the database, the product groups are stored in the table <code>Product</code> with the flag <code>is_product_group</code> set to <code>true</code>.</p>"},{"location":"getting_started/data_model/#product","title":"Product","text":"<p>A <code>Product</code> is the representation of the system that is checked for vulnerabilities.</p>"},{"location":"getting_started/data_model/#observation","title":"Observation","text":"<p>An <code>Observation</code> is something that has been discovered by a vulnerability scanner. Not every observation is actually a vulnerability. An assessment can show it is e.g. a false positive or not applicable in the current context.</p> <p>Every <code>Observation</code> belongs to exactly one product.</p>"},{"location":"getting_started/data_model/#branch","title":"Branch","text":"<p>Software development often uses branches in the source code repository. Vulnerability scanners can run for multiple branches of a product and observations can be viewed and managed by branch. See more in Working with branches.</p>"},{"location":"getting_started/data_model/#service","title":"Service","text":"<p>A <code>Service</code> is a self-contained piece of functionality of a product. Can be something like a microservice or <code>backend</code> or <code>frontend</code>.   </p>"},{"location":"getting_started/data_model/#vulnerability-check","title":"Vulnerability Check","text":"<p>An import for one product, one branch and one file name resp. one API configuration is a so-called vulnerability check. See more in Import algorithm.</p>"},{"location":"getting_started/data_model/#parser","title":"Parser","text":"<p>SecObserve can parse a variety of data formats, written by vulnerability scanners. Besides file-based parsers, SecObserve implements API-based parsers as well, which get data directly from a system via REST calls. See more about vulnerability scanners and parsers on Supported scanners.</p>"},{"location":"getting_started/data_model/#observation-log","title":"Observation Log","text":"<p>Every change of the severity or the status of an observation is recorded in the <code>Observation Log</code>.</p>"},{"location":"getting_started/data_model/#reference","title":"Reference","text":"<p><code>References</code> are links to further information about the observation. They are imported with the observation.</p>"},{"location":"getting_started/data_model/#evidence","title":"Evidence","text":"<p><code>Evidences</code> are extracts from the scan reports showing the basis on which the observation was created.</p>"},{"location":"getting_started/data_model/#product-rule-general-rule","title":"Product Rule / General Rule","text":"<p>Rules can change the severity or the status of an observation can be changed during the import. An example would be to set a risk acceptance automatically for observations that shall not be fixed. <code>General Rules</code> will be applied for all products, while <code>Product Rules</code> are only valid for one product. See more about rules on Rule engine.</p>"},{"location":"getting_started/data_model/#api-configuration","title":"API Configuration","text":"<p>Parsers who get data from vulnerability scanners via a REST API need a configuration how to access the API (URL, API key, ...). The <code>API Configuration</code> is set per product.</p>"},{"location":"getting_started/data_model/#product-member","title":"Product Member","text":"<p><code>Product Members</code> define who has access to a product. Depending on the role of a user for a product, they have more or less functionality available, see more on Users and permissions.</p>"},{"location":"getting_started/features/","title":"Features","text":""},{"location":"getting_started/features/#core","title":"Core","text":"Feature Supported Flexible data model with products, product groups and services Observations with a wide range of information Multiple branches per product Automatic resolution of fixed vulnerabilities Identification and management of duplicates Manual assessment of severity and status Rule based assessment of severity and status Security gates Actual and weekly metrics"},{"location":"getting_started/features/#integrations","title":"Integrations","text":"Feature Supported Import from many SAST, SCA, DAST, infrastructure and secrets scanners GitLab CI integration of scanners with predefined templatesGitHub integration of scanners with predefined actions Data enrichment from Exploit Prediction Scoring System (EPSS) Direct link to source code Export vulnerabilities to issue trackers (Jira, GitLab, GitHub) Export of data to Microsoft Excel and CSV Export metrics to CodeCharta Notifications to Microsoft Teams, Slack and email Links to additional information about vulnerabilities and components REST API"},{"location":"getting_started/features/#access-control","title":"Access Control","text":"Feature Supported Built-in user management OpenID Connect integration Internal, external and admin users Role-based access control"},{"location":"getting_started/features/#installation-and-upgrading","title":"Installation and Upgrading","text":"Feature Supported Installation with Docker Compose Supported databases: PostgreSQL and MySQL Flexible configuration Automatic database migration during upgrades"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#docker-compose","title":"Docker Compose","text":"<p>SecObserve provides 2 Docker Compose files as templates for productive use: <code>docker-compose-prod-mysql.yml</code> and <code>docker-compose-prod-postgres.yml</code>. Both start Traefik as an edge router as well as the SecObserve frontend and backend plus a database (either MySQL or PostgreSQL).</p> <p>Without any changes to the Docker Compose file, 3 URL's are available:</p> <ul> <li>Frontend: http://secobserve.localhost</li> <li>Backend: http://secobserve-backend.localhost (base URL)</li> <li>Traefik: http://traefik.localhost (dashboard)</li> </ul> docker-compose-prod-postgres.yml<pre><code>version: \"3\"\n\nvolumes:\n  prod_postgres_data:\n\nnetworks:\n  traefik:\n  database:\n\nservices:\n\n  traefik:\n    image: \"traefik:v3.0\"\n    container_name: \"traefik\"\n    command:\n      - \"--log.level=INFO\"\n      - \"--api.dashboard=true\"\n      - \"--providers.docker=true\"\n      - \"--providers.docker.exposedbydefault=false\"\n      - \"--entrypoints.web.address=:80\"\n    labels:\n      - \"traefik.enable=true\"\n      # - \"traefik.http.middlewares.traefik-ipallowlist.ipallowlist.sourcerange=172.18.0.1/24\"\n      # - \"traefik.http.routers.api.middlewares=traefik-ipallowlist@docker\"\n      - \"traefik.http.routers.api.entrypoints=web\"\n      - \"traefik.http.routers.api.rule=Host(`traefik.localhost`)\"\n      - \"traefik.http.routers.api.service=api@internal\"\n    ports:\n      - \"80:80\"\n    volumes:\n      - \"/var/run/docker.sock:/var/run/docker.sock:ro\"\n    networks:\n      - default\n\n  frontend:\n    image: maibornwolff/secobserve-frontend:1.7.0\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.frontend.rule=Host(`secobserve.localhost`)\"\n      - \"traefik.http.routers.frontend.entrypoints=web\"\n    environment:\n      API_BASE_URL: ${SO_API_BASE_URL:-http://secobserve-backend.localhost/api}\n      OIDC_ENABLE: ${SO_OIDC_ENABLE:-false}\n      OIDC_AUTHORITY: ${SO_OIDC_AUTHORITY:-dummy}\n      OIDC_CLIENT_ID: ${SO_OIDC_CLIENT_ID:-dummy}\n      OIDC_REDIRECT_URI: ${SO_OIDC_REDIRECT_URI:-http://secobserve.localhost}\n      OIDC_POST_LOGOUT_REDIRECT_URI: ${SO_OIDC_POST_LOGOUT_REDIRECT_URI:-http://secobserve.localhost}\n    networks:\n      - traefik\n\n  backend:\n    image: maibornwolff/secobserve-backend:1.7.0\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.backend.rule=Host(`secobserve-backend.localhost`)\"\n      - \"traefik.http.routers.backend.entrypoints=web\"\n    depends_on:\n      - postgres\n    environment:\n      # --- Admin user ---\n      ADMIN_USER: ${SO_ADMIN_USER:-admin}\n      ADMIN_PASSWORD: ${SO_ADMIN_PASSWORD:-admin}\n      ADMIN_EMAIL: ${SO_ADMIN_EMAIL:-admin@example.com}\n      # --- Database ---\n      DATABASE_ENGINE: ${SO_DATABASE_ENGINE:-django.db.backends.postgresql}\n      DATABASE_HOST: ${SO_DATABASE_HOST:-postgres}\n      DATABASE_PORT: ${SO_DATABASE_PORT:-5432}\n      DATABASE_DB: ${SO_DATABASE_DB:-secobserve}\n      DATABASE_USER: ${SO_DATABASE_USER:-secobserve}\n      DATABASE_PASSWORD: ${SO_DATABASE_PASSWORD:-secobserve}\n      # --- Security ---\n      ALLOWED_HOSTS: ${SO_ALLOWED_HOSTS:-secobserve-backend.localhost}\n      CORS_ALLOWED_ORIGINS: ${SO_CORS_ALLOWED_ORIGINS:-http://secobserve.localhost}\n      DJANGO_SECRET_KEY: ${SO_DJANGO_SECRET_KEY:-NxYPEF5lNGgk3yonndjSbwP77uNJxOvfKTjF5aVBqsHktNlf1wfJHHvJ8iifk32r}\n      FIELD_ENCRYPTION_KEY: ${SO_FIELD_ENCRYPTION_KEY:-DtlkqVb3wlaVdJK_BU-3mB4wwuuf8xx8YNInajiJ7GU=}\n      # --- OpenID Connect ---\n      OIDC_AUTHORITY: ${SO_OIDC_AUTHORITY:-}\n      OIDC_CLIENT_ID: ${SO_OIDC_CLIENT_ID:-}\n      OIDC_USERNAME: ${SO_OIDC_USERNAME:-}\n      OIDC_FIRST_NAME: ${SO_OIDC_FIRST_NAME:-}\n      OIDC_LAST_NAME: ${SO_OIDC_LAST_NAME:-}\n      OIDC_FULL_NAME: ${SO_OIDC_FULL_NAME:-}\n      OIDC_EMAIL: ${SO_OIDC_EMAIL:-}\n    command: /start\n    networks:\n      - traefik\n      - database\n\n  postgres:\n    image: postgres:15.2-alpine\n    environment:\n      POSTGRES_DB: ${SO_POSTGRES_DB:-secobserve}\n      POSTGRES_USER: ${SO_POSTGRES_USER:-secobserve}\n      POSTGRES_PASSWORD: ${SO_POSTGRES_PASSWORD:-secobserve}\n    volumes:\n      - prod_postgres_data:/var/lib/postgresql/data\n    networks:\n      - database\n</code></pre>"},{"location":"getting_started/installation/#configuration-for-traefik","title":"Configuration for Traefik","text":"<ul> <li>The Traefik dashboard should either be configured with authentication or disabled, see The Dashboard.</li> <li>Encrypted communiction should be configured for frontend and backend. Traefik supports given certificates and automatic configuration with Let's Encrypt, see HTTPS &amp; TLS.</li> </ul>"},{"location":"getting_started/installation/#configuration-for-secobserve","title":"Configuration for SecObserve","text":"<p>The Docker Compose file sets default values for the SecObserve configuration, so that the containers can run out of the box. All default values can be overriden, by setting respective environment variables in the shell before starting Docker Compose. To avoid name collisions, the environment variables in the shell need to have a <code>SO_</code> prefix in front of the name as it is stated in Configuration.</p> <p>Some values should be changed for productive use, to avoid using the default values for secrets:</p> <ul> <li><code>SO_ADMIN_PASSWORD</code></li> <li><code>SO_DATABASE_PASSWORD</code></li> <li><code>SO_DJANGO_SECRET_KEY</code></li> <li><code>SO_FIELD_ENCRYPTION_KEY</code></li> </ul>"},{"location":"getting_started/installation/#startup","title":"Startup","text":"<ul> <li>The database structure is initialized with the first start of the backend container.</li> <li>The URLs for frontend and backend are available after approximately 30 seconds, after the healthcheck of the containers has been running for the first time.</li> </ul>"},{"location":"getting_started/upgrading/","title":"Upgrading","text":""},{"location":"getting_started/upgrading/#generic-upgrade-procedure","title":"Generic upgrade procedure","text":"<ul> <li> <p>Frontend and backend shall always be started with the same version number. </p> </li> <li> <p>The Docker Compose <code>docker-compose-prod-mysql.yml</code> and <code>docker-compose-prod-postgres.yml</code> in the GitHub repository always use the most recent released version of SecObserve.</p> </li> <li> <p>The database structure will automatically be updated to the reflect the latest changes, when the backend container gets started. Always make a backup of your database before upgrading, in case something should go wrong.</p> </li> <li> <p>There will be specific upgrade instructions if necessary, e.g. when there are new configuration parameters.</p> </li> </ul>"},{"location":"getting_started/upgrading/#release-150","title":"Release 1.5.0","text":"<p>Breaking changes</p> <ul> <li>The tag of the docker image is not part of the identity hash anymore, to allow updates of the docker image within a vulnerability check without creating a new observation.</li> </ul>"},{"location":"getting_started/upgrading/#release-130","title":"Release 1.3.0","text":"<p>Breaking changes</p> <ul> <li>The ZAP project has had a rebranding as a result of the move to the Software Security Project. To reflect this, the name of the parser has been changed from <code>OWASP ZAP</code> to <code>ZAP</code>. The GitLab template and GitLab action for <code>ZAP</code> have been renamed as well. These changes are not backwards compatible, so you need to update your configuration files and pipelines.</li> </ul>"},{"location":"getting_started/upgrading/#release-110","title":"Release 1.1.0","text":"<p>Breaking changes</p> <ul> <li>When OIDC authentication is used, the environment variable <code>OIDC_CLIENT_ID</code> needs to be set for the backend as well. See Configuration and OpenID Connect authentication for details.</li> </ul>"},{"location":"getting_started/upgrading/#release-100","title":"Release 1.0.0","text":"<p>Breaking changes</p> <ul> <li>SecObserve now supports different OpenID Connect providers for authentication and the Microsoft specific dependencies have been removed. Thus the <code>AAD_</code> configuration parameters are not valid anymore and have been replaced with <code>OIDC_</code> parameters, see Configuration and OpenID Connect authentication for details.</li> </ul>"},{"location":"getting_started/upgrading/#release-099","title":"Release 0.9.9","text":"<p>Breaking changes</p> <ul> <li>The value of the configuration parameter <code>MYSQL_AZURE</code> has been changed from <code>true</code> to <code>flexible</code> or <code>single</code>, depending on the type of Azure Database for MySQL. See Configuration for details.</li> </ul>"},{"location":"getting_started/upgrading/#release-091","title":"Release 0.9.1","text":"<p>Breaking changes</p> <ul> <li> <p>The SSLyze parser has been replaced by the CryptoLyzer parser due to licensing reasons. Even though the SSLyze parser may still be seen in the list of parsers, you cannot use it for imports anymore. The CryptoLyter parser generates the same kind of results, adding information about signature algorithms.</p> </li> <li> <p>The project name <code>secobserve_prod</code> has been set in <code>docker-compose-prod-mysql.yml</code> and <code>docker-compose-prod-postgres.yml</code>. This was necessary to assign defined network names, but it changes the name of the database volume. You need to dump the database content to a file before the upgrade and restore it after the upgrade.</p> </li> </ul>"},{"location":"integrations/codecharta/","title":"CodeCharta","text":"<p>CodeCharta is a tool to visualize software quality. It allows you to import and combine metrics from various sources like SonarQube, Svn, Git or directly from your source code. The visualization makes the quality of a code base tangible and can be used to gain insights and communicate with stakeholders.</p> <p></p>"},{"location":"integrations/codecharta/#export-data-from-secobserve","title":"Export data from SecObserve","text":"<p>When showing a product, there is an <code>Export</code> button. When clicking it, it shows a menu including an option to export the metrics about observations of this product to CodeCharta:</p> <p></p> <p>This export produces a CSV file with severity metrics for all open observations that have a source file as an origin.</p>"},{"location":"integrations/codecharta/#process-data-for-codecharta","title":"Process data for CodeCharta","text":"<p>The CSV file needs to be converted to CodeCharta's JSON format. First the analysis tools of CodeCharta need to be installed. Then the exported metrics are converted like this:</p> <pre><code>ccsh csvimport secobserve_codecharta_metrics.csv -o secobserve_codecharta_metrics.cc.json\n</code></pre> <p>This produces the file <code>secobserve_codecharta_metrics.cc.json.gz</code>. This file can be visualized by CodeCharta, but contains only the source files with vulnerabilities.</p> <p>To get a better picture of the whole system, this file can be combined with an export from SonarQube or the Source Code Parser.</p> <p>When SonarQube is used, the export from SonarQube will include an additional node with the id of the project, that needs to be removed:</p> <pre><code>ccsh modify -f root/csec_secobserve -t root -o secobserve_sonarqube_modified.cc.json.gz secobserve_sonarqube.cc.json.gz\n</code></pre> <p>Now the results from SonarQube and SecObserve can be merged:</p> <pre><code>ccsh merge secobserve_sonarqube_modified.cc.json secobserve_codecharta_metrics.cc.json.gz -o secobserve.cc.json.gz\n</code></pre>"},{"location":"integrations/codecharta/#visualize-data-in-codecharta","title":"Visualize data in CodeCharta","text":"<p>The resulting file <code>secobserve.cc.json.gz</code> can now be visualized using https://maibornwolff.github.io/codecharta/visualization/app/index.html. These SecObserve metrics are included:</p> <ul> <li>vulnerabilities_total</li> <li>vulnerabilities_critical</li> <li>vulnerabilities_high</li> <li>vulnerabilities_medium</li> <li>vulnerabilities_low</li> <li>vulnerabilities_none</li> <li>vulnerabilities_unkown</li> <li>vulnerabilities_high_and_above</li> <li>vulnerabilities_medium_and_above</li> <li>vulnerabilities_low_and_above</li> </ul>"},{"location":"integrations/epss/","title":"Exploit Prediction Scoring System (EPSS)","text":"<p>The Exploit Prediction Scoring System (EPSS) is a data-driven effort for estimating the likelihood (probability) that a software vulnerability will be exploited in the wild. The EPSS model produces a probability score between 0 and 1 (0 and 100%) for all CVE vulnerabilities. The higher the score, the greater the probability that a vulnerability will be exploited. Additionally percentiles are calculated, which are a direct transformation from probabilities and provide a measure of an EPSS probability relative to all other scores. That is, the percentile is the proportion of all values less than or equal to the current rank. A good overview of EPSS scores and EPSS percentiles is given in Probability, Percentiles, and Binning - How to understand and interpret EPSS Scores. The EPSS data is updated daily.</p> <p>SecObserve imports the EPSS data and updates all observations with a CVE value with the EPSS score and EPSS percentile regularly. After an import of a vulnerability scan, all observations with a CVE number contained in the import are updated with the EPSS score and EPSS percentile as well.</p>"},{"location":"integrations/epss/#configuration","title":"Configuration","text":"<p>Per default the task to import the EPSS data and update the observations is scheduled to run every night at 03:00 UTC time. This default can be changed by administrators via the admin user interface. The expressions for <code>BACKGROUND_EPSS_IMPORT_CRONTAB_MINUTES</code> and <code>BACKGROUND_EPSS_IMPORT_CRONTAB_HOURS</code> have to be valid values according to https://huey.readthedocs.io/en/latest/api.html#crontab:</p> <ul> <li><code>*</code> = every distinct value (every minute, every hour)</li> <li><code>*/n</code> = run every <code>n</code> times, i.e. hours=\u2019*/4\u2019 == 0, 4, 8, 12, 16, 20</li> <li><code>n</code> = run every <code>n</code> (minutes 0 - 60, hours 0 - 24)</li> <li><code>m-n</code> = run every time m..n</li> <li><code>m,n</code> = run on m and n</li> </ul> <p>Hours are always in UTC time.</p>"},{"location":"integrations/github_actions_and_templates/","title":"GitHub actions and GitLab CI templates","text":"<p>Integrating vulnerability scanners in a CI/CD pipeline can be cumbersome. Every tool is different to install and has different parameters. Our repository of GitHub actions and GitLab CI templates makes this process very straightforward, with a unified way to start the tools. The tools in the template repository will be updated regularly, so that all the latest features and bugfixes are available.</p> <p>All actions and templates run the scanner, import the results into SecObserve and make the report available as an artifact.</p> <p>The actions and the templates are stored in the repository https://github.com/MaibornWolff/secobserve_actions_templates.</p>"},{"location":"integrations/github_actions_and_templates/#variables","title":"Variables","text":"<p>Most of the actions and templates use the same set of variables:</p> Variable Optionality Description Scanning <code>TARGET</code> mandatory The target to be scanned, often it is a path of the filesystem, but can be a Docker image, an URL or others. <code>REPORT_NAME</code> mandatory The name of the report to be written. It will be saved as an artifact. <code>RUN_DIRECTORY</code> optional The directory where to run the scanner, only to be used when the <code>TARGET</code> is a path. <code>FURTHER_PARAMETERS</code> optional Further parameters to be given to the scanner. <code>CONFIGURATION</code> mandatory, only for Semgrep Configuration to be used with Semgrep. <code>RULES</code> optional, only for DrHeader Custom rules to be used with DrHeader. <code>SCRIPT</code> optional, only for ZAP Script to be executed, default is <code>zap-baseline.py</code>. Importing <code>SO_UPLOAD</code> optional No upload of observations into SecObserve if value is not <code>true</code>, default is <code>true</code>. <code>SO_API_BASE_URL</code> mandatory Base URL of the SecObserve backend, e.g. <code>https://secobserve-backend.example.com</code>. <code>SO_API_TOKEN</code> mandatory API token of the user to be used for the import. The users needs at least the <code>Upload</code> role. <code>SO_PRODUCT_NAME</code> mandatory Name of the product which observations are imported. The product has to exist before starting the import. <code>SO_BRANCH_NAME</code> optional Name of the branch in the source code repository. <code>SO_ORIGIN_SERVICE</code> optional Service name to be set for all imported observations. <code>SO_ORIGIN_DOCKER_IMAGE_NAME_TAG</code> optional Name:Tag of Docker image to be set for all imported observations. <code>SO_ORIGIN_ENDPOINT_URL</code> optional URL of endpoint to be set for all imported observations."},{"location":"integrations/github_actions_and_templates/#available-actions-and-templates","title":"Available actions and templates","text":"Scanner GitHub Action GitLab CI Template License Bandit <code>actions/SAST/bandit</code> <code>templates/SAST/bandit.yml</code> Apache 2.0 ESLint <code>actions/SAST/eslint</code> <code>templates/SAST/eslint.yml</code> MIT Semgrep <code>actions/SAST/semgrep</code> <code>templates/SAST/semgrep.yml</code> LGPL 2.1 Checkov <code>actions/SAST/checkov</code> <code>templates/SAST/checkov.yml</code> Apache 2.0 KICS <code>actions/SAST/kics</code> <code>templates/SAST/kics.yml</code> Apache 2.0 tfsec <code>actions/SAST/tfsec</code> <code>templates/SAST/tfsec.yml</code> MIT Trivy <code>actions/SCA/trivy_config</code> <code>templates/SCA/trivy_config.yml</code> Apache 2.0 Grype <code>actions/SCA/grype_image</code> <code>templates/SCA/grype_image.yml</code> Apache 2.0 Trivy <code>actions/SCA/trivy_filesystem</code> <code>templates/SCA/trivy_filesystem.yml</code> Apache 2.0 Trivy <code>actions/SCA/trivy_image</code> <code>templates/SCA/trivy_image.yml</code> Apache 2.0 Gitleaks <code>actions/secrets/gitleaks</code> <code>templates/secrets/gitleaks.yml</code> MIT CryptoLyzer <code>actions/DAST/cryptolyzer</code> <code>templates/DAST/cryptolyzer.yml</code> MPL 2.0 DrHeader <code>actions/DAST/drheader</code> <code>templates/DAST/drheader.yml</code> MIT ZAP <code>actions/DAST/zap</code> <code>templates/DAST/zap.yml</code> Apache 2.0 <p>All GitHub actions and GitLab CI templates use a pre-built Docker image that contains all scanners and the SecObserve importer.</p>"},{"location":"integrations/github_actions_and_templates/#examplary-workflow-for-github-actions","title":"Examplary workflow for GitHub actions","text":"<p>Tip</p> <p>The mandatory variables for importing (<code>SO_API_BASE_URL</code>, <code>SO_API_TOKEN</code>, <code>SO_PRODUCT_NAME</code>) can be set as secrets and variables in the settings of the project in GitHub.</p> <pre><code>name: Vulnerability checks\n\non: [push]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n\n      - name: Run Bandit\n        uses: MaibornWolff/secobserve_actions_templates/actions/SAST/bandit@main\n        with:\n          target: 'dd_import'\n          report_name: 'dd_import_bandit.sarif'\n          so_api_base_url: ${{ vars.SO_API_BASE_URL }}\n          so_api_token: ${{ secrets.SO_API_TOKEN }}\n          so_product_name: ${{ vars.SO_PRODUCT_NAME }}\n\n      - name: Run Semgrep\n        uses: MaibornWolff/secobserve_actions_templates/actions/SAST/semgrep@main\n        with:\n          target: 'dd_import'\n          report_name: 'dd_import_semgrep.json'\n          configuration: 'r/python'\n          so_api_base_url: ${{ vars.SO_API_BASE_URL }}\n          so_api_token: ${{ secrets.SO_API_TOKEN }}\n          so_product_name: ${{ vars.SO_PRODUCT_NAME }}\n\n      - name: Run KICS\n        uses: MaibornWolff/secobserve_actions_templates/actions/SAST/kics@main\n        with:\n          target: '.'\n          report_name: 'dd_import_kics.sarif'\n          so_api_base_url: ${{ vars.SO_API_BASE_URL }}\n          so_api_token: ${{ secrets.SO_API_TOKEN }}\n          so_product_name: ${{ vars.SO_PRODUCT_NAME }}\n\n      - name: Run Checkov\n        uses: MaibornWolff/secobserve_actions_templates/actions/SAST/checkov@main\n        with:\n          target: '.'\n          report_name: 'dd_import_checkov.sarif'\n          so_api_base_url: ${{ vars.SO_API_BASE_URL }}\n          so_api_token: ${{ secrets.SO_API_TOKEN }}\n          so_product_name: ${{ vars.SO_PRODUCT_NAME }}\n\n      - name: Run Trivy image\n        uses: MaibornWolff/secobserve_actions_templates/actions/SCA/trivy_image@main\n        with:\n          target: 'maibornwolff/dd-import:latest'\n          report_name: 'dd_import_trivy_image.json'\n          so_api_base_url: ${{ vars.SO_API_BASE_URL }}\n          so_api_token: ${{ secrets.SO_API_TOKEN }}\n          so_product_name: ${{ vars.SO_PRODUCT_NAME }}\n\n      - name: Run Grype image\n        uses: MaibornWolff/secobserve_actions_templates/actions/SCA/grype_image@main\n        with:\n          target: 'maibornwolff/dd-import:latest'\n          report_name: 'dd_import_grype_image.json'\n          so_api_base_url: ${{ vars.SO_API_BASE_URL }}\n          so_api_token: ${{ secrets.SO_API_TOKEN }}\n          so_product_name: ${{ vars.SO_PRODUCT_NAME }}\n\n      - name: Run Gitleaks\n        uses: MaibornWolff/secobserve_actions_templates/actions/secrets/gitleaks@main\n        with:\n          report_name: 'dd_import_gitleaks.sarif'\n          so_api_base_url: ${{ vars.SO_API_BASE_URL }}\n          so_api_token: ${{ secrets.SO_API_TOKEN }}\n          so_product_name: ${{ vars.SO_PRODUCT_NAME }}\n\n      - name: Upload results\n        uses: actions/upload-artifact@v3\n        with:\n          name: secobserve\n          path: |\n            dd_import_bandit.sarif\n            dd_import_semgrep.json\n            dd_import_kics.sarif\n            dd_import_checkov.sarif\n            dd_import_trivy_image.json\n            dd_import_grype_image.json\n            dd_import_gitleaks.sarif\n</code></pre>"},{"location":"integrations/github_actions_and_templates/#examplary-pipeline-for-gitlab-ci-templates","title":"Examplary pipeline for GitLab CI templates","text":"<p>Tip</p> <p>The mandatory variables for importing (<code>SO_API_BASE_URL</code>, <code>SO_API_TOKEN</code> and <code>SO_PRODUCT_NAME</code>) can be set as variables in the CI/CD settings of the project in GitLab. Then they don't need to be set in each job.</p> <pre><code>include:\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/DAST/drheader.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/DAST/cryptolyzer.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/SAST/bandit.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/SAST/checkov.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/SAST/eslint.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/SAST/kics.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/SAST/semgrep.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/SCA/grype_image.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/SCA/trivy_filesystem.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/SCA/trivy_image.yml\"\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/secrets/gitleaks.yml\"\n\ngrype_image_backend:\n  extends: .grype_image\n  variables:\n    TARGET: \"$BACKEND_IMAGE_FULL_NAME_BRANCH\"\n    REPORT_NAME: \"grype_backend_image.json\"\n    SO_ORIGIN_SERVICE: \"backend\"\n\ngrype_image_frontend:\n  extends: .grype_image\n  variables:\n    TARGET: \"$FRONTEND_IMAGE_FULL_NAME_BRANCH\"\n    REPORT_NAME: \"grype_frontend_image.json\"\n    SO_ORIGIN_SERVICE: \"frontend\"\n\ntrivy_image_backend:\n  extends: .trivy_image\n  variables:\n    TARGET: \"$BACKEND_IMAGE_FULL_NAME_BRANCH\"\n    REPORT_NAME: \"trivy_backend_image.json\"\n    SO_ORIGIN_SERVICE: \"backend\"\n\ntrivy_image_frontend:\n  extends: .trivy_image\n  variables:\n    TARGET: \"$FRONTEND_IMAGE_FULL_NAME_BRANCH\"\n    REPORT_NAME: \"trivy_frontend_image.json\"\n    SO_ORIGIN_SERVICE: \"frontend\"\n\ntrivy_filesystem_frontend:\n  extends: .trivy_filesystem\n  variables:\n    TARGET: \"frontend/package-lock.json\"\n    REPORT_NAME: \"trivy_frontend_npm.json\"\n    SO_ORIGIN_SERVICE: \"frontend\"\n  needs: []\n\nbandit_backend:\n  extends: .bandit\n  variables:\n    TARGET: \"backend\"\n    REPORT_NAME: \"bandit_backend.sarif\"\n    SO_ORIGIN_SERVICE: \"backend\"\n  needs: []\n\neslint_frontend:\n  extends: .eslint\n  variables:\n    RUN_DIRECTORY: \"frontend\"\n    TARGET: \"src\"\n    REPORT_NAME: \"eslint_frontend.sarif\"\n    SO_ORIGIN_SERVICE: \"frontend\"\n  needs: []\n\nsemgrep_backend:\n  extends: .semgrep\n  variables:\n    CONFIGURATION: \"r/python\"\n    TARGET: \"backend\"\n    REPORT_NAME: \"semgrep_backend.json\"\n    SO_ORIGIN_SERVICE: \"backend\"\n  needs: []\n\nsemgrep_frontend:\n  extends: .semgrep\n  variables:\n    CONFIGURATION: \"r/typescript\"\n    TARGET: \"frontend/src\"\n    REPORT_NAME: \"semgrep_frontend.json\"\n    SO_ORIGIN_SERVICE: \"frontend\"\n  needs: []\n\ngitleaks:\n  extends: .gitleaks\n  variables:\n    REPORT_NAME: \"gitleaks.sarif\"\n  needs: []\n\n\ncheckov:\n  extends: .checkov\n  variables:\n    TARGET: \".\"\n    REPORT_NAME: \"checkov.sarif\"\n  needs: []\n\nkics:\n  extends: .kics\n  variables:\n    TARGET: \".\"\n    REPORT_NAME: \"kics.sarif\"\n  needs: []\n\ndrheader:\n  extends: .drheader\n  variables:\n    TARGET: \"https://secobserve.example.com\"\n    REPORT_NAME: \"drheader.json\"\n    SO_ORIGIN_ENDPOINT_URL: \"https://secobserve.example.com\"\nneeds: []\n\ncryptolyzer:\n  extends: .cryptolyzer\n  variables:\n    TARGET: \"secobserve.example.com\"\n    REPORT_NAME: \"cryptolyzer.json\"\n  needs: []\n</code></pre>"},{"location":"integrations/github_actions_and_templates/#using-a-configuration-file","title":"Using a configuration file","text":"<p>Using multiple vulnerability scanners makes the pipeline quite complex. To make the pipeline smaller, a configuration file can be used to define the scanners to be used with their parameters. The configuration file is a YAML file with sections per scanner and one section for the import into SecObserve.</p>"},{"location":"integrations/github_actions_and_templates/#example-pipeline-for-github","title":"Example pipeline for GitHub","text":"<pre><code>name: Check for vulnerabilities in the code\n\non: [push]\n\npermissions: read-all\n\njobs:\n  check_vulnerabilities:\n\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Run vulnerability scanners\n        uses: MaibornWolff/secobserve_actions_templates/actions/vulnerability_scanner@main\n        with:\n          so_configuration: 'so_configuration.yml'\n          SO_API_TOKEN: ${{ secrets.SO_API_TOKEN }}\n</code></pre>"},{"location":"integrations/github_actions_and_templates/#example-pipeline-for-gitlab","title":"Example pipeline for GitLab","text":"<pre><code>include:\n  - \"https://raw.githubusercontent.com/MaibornWolff/secobserve_actions_templates/main/templates/vulnerability_scanner.yml\"\n\nvulnerability_scans:\n  stage: test\n  extends: .vulnerability_scanner\n  variables:\n    SO_CONFIGURATION: \"so_configuration.yml\"\n  needs: []\n</code></pre>"},{"location":"integrations/github_actions_and_templates/#example-configuration-file","title":"Example configuration file","text":"<pre><code>bandit_backend:\n  SCANNER: bandit\n  RUN_DIRECTORY: \".\"\n  TARGET: backend\n  REPORT_NAME: bandit_backend.sarif\n  SO_ORIGIN_SERVICE: backend\n  SO_BRANCH_NAME: $GITHUB_REF_NAME\n\ncheckov:\n  SCANNER: checkov\n  RUN_DIRECTORY: \".\"\n  TARGET: \".\"\n  REPORT_NAME: checkov.sarif\n  SO_BRANCH_NAME: $GITHUB_REF_NAME\n\neslint_frontend:\n  SCANNER: eslint\n  RUN_DIRECTORY: \"frontend\"\n  TARGET: \"src\"\n  REPORT_NAME: \"eslint_frontend.sarif\"\n  SO_ORIGIN_SERVICE: \"frontend\"\n  SO_BRANCH_NAME: $GITHUB_REF_NAME\n\ngitleaks:\n  SCANNER: gitleaks\n  RUN_DIRECTORY: \".\"\n  REPORT_NAME: \"gitleaks.sarif\"\n  SO_BRANCH_NAME: $GITHUB_REF_NAME\n\nkics:\n  SCANNER: kics\n  RUN_DIRECTORY: \".\"\n  TARGET: \".\"\n  REPORT_NAME: \"kics.sarif\"\n  SO_BRANCH_NAME: $GITHUB_REF_NAME\n\nsemgrep_backend:\n  SCANNER: semgrep\n  RUN_DIRECTORY: \".\"\n  CONFIGURATION: \"r/python\"\n  TARGET: \"backend\"\n  REPORT_NAME: \"semgrep_backend.sarif\"\n  SO_ORIGIN_SERVICE: \"backend\"\n  SO_BRANCH_NAME: $GITHUB_REF_NAME\n\nsemgrep_frontend:\n  SCANNER: semgrep\n  RUN_DIRECTORY: \".\"\n  CONFIGURATION: \"r/typescript\"\n  TARGET: \"frontend/src\"\n  REPORT_NAME: \"semgrep_frontend.sarif\"\n  SO_ORIGIN_SERVICE: \"frontend\"\n  SO_BRANCH_NAME: $GITHUB_REF_NAME\n\ntrivy_filesystem_frontend:\n  SCANNER: trivy_filesystem\n  RUN_DIRECTORY: \".\"\n  TARGET: \"frontend/package-lock.json\"\n  REPORT_NAME: \"trivy_frontend_npm.json\"\n  SO_ORIGIN_SERVICE: \"frontend\"\n  SO_BRANCH_NAME: $GITHUB_REF_NAME\n\nimporter:\n  SO_UPLOAD: \"true\"\n  SO_API_BASE_URL: https://secobserve-backend.example.com\n  SO_PRODUCT_NAME: SecObserve\n</code></pre>"},{"location":"integrations/github_actions_and_templates/#real-life-examples","title":"Real life examples","text":"<p>Some real life examples can be found in the SecObserve GitHub repository:</p> <ul> <li>so_configuration_code.yml used in pipeline check_vulnerabilities.yml</li> <li>so_configuration_images.yml used in pipeline build_push_dev.yml</li> <li>so_configuration_endpoints.yml used in pipeline build_push_release.yml</li> </ul>"},{"location":"integrations/issue_trackers/","title":"Issue trackers","text":"<p>Developers and product owners will typically document the development tasks in an issue tracker and move these issues in a Scrum or Kanban board. Therefore, even though the assessment of observations is done in SecObserve, it may be useful to transfer them to an issue tracker as well. SecObserve currently supports automatic creation of issues in GitHub, GitLab and Jira (Cloud). </p> <p>The synchronization of issues is an unidirectional process:</p> <ul> <li>A new observation will be pushed as an open issue, if its branch is set to the default branch of the product.</li> <li>If the observation changes its priority or description, the issue will be updated accordingly.</li> <li>If the observation changes the status from Open to another status by an assessment in SecObserve, the issue will be closed.</li> <li>If the observation gets the status Resolved in SecObserve because it is not found in the results anymore, the issue will be closed as well.</li> <li>If the observation is reopened in SecObserve by an assessment or because it has been found in the results again, the issue will be opened again.</li> <li>If the observation is deleted in SecObserve, the issue will be closed.</li> </ul> <p>If the issue is closed or deleted in the issue tracker, it will be reopened or recreated with the next import of results in SecObserve, when the respective observation was still found and still has the status Open in SecObserve.</p> <p>The parameters for the issue tracker integration are set in the product:</p> <p></p> Active Issues will only be pushed, if this parameter is set. Type Either GitHub or GitLab or Jira Base URL The base URL of the issue tracker. For GitHub it is <code>https://api.github.com</code>, for a self hosted GitLab it will be something like <code>https://gitlab.example.com</code>, for Jira it is <code>https:\\\\{organization_name}.atlassian.net</code>. API key An API key must be created in the issue tracker, having the permissions to create and update issues. Project id The path of the repository in its URL in GitHub or GitLab, e.g. <code>MaibornWolff/SecObserve</code>. For Jira it is the key of the project. Labels A comma separated list of labels, that will be set for the issue. Additional labels can be set in the issue tracker, they will be preserved when the issue is updated. Minimum severity (optional) Issues will only be exported for observations with a severity that is higher or the same. Username (only for Jira) The REST API of Jira needs an authentication with username and API key. Issue type (only for Jira) The issue type to be created. Closed status (only for Jira) The status to be set when an issue is closed. <p>Issues are created or updated by an asynchronous background process after the import or the assessment of an observation has finished. If problems should occur during the transfer, a notification is send, see Notifications.</p> <p>Tip</p> <p>Issues shouldn't be created when observations are imported the first time for a vulnerability scannner. First the number of observations should be minimized with settings of the vulnerability scanner of rules within SecObserve, before pushing issues to an issue tracker.</p>"},{"location":"integrations/issue_trackers/#issue-in-github","title":"Issue in GitHub","text":""},{"location":"integrations/issue_trackers/#issue-in-gitlab","title":"Issue in GitLab","text":""},{"location":"integrations/issue_trackers/#issue-in-jira-cloud","title":"Issue in Jira (Cloud)","text":""},{"location":"integrations/links/","title":"Links to additional information","text":""},{"location":"integrations/links/#references","title":"References","text":"<p>Most of the vulnerability scanners include references to further information about the vulnerabilities. These references are imported with the observation and can be accessed by clicking on the link icon in the <code>References</code> box, on the right side of the Observation view.</p> <p></p>"},{"location":"integrations/links/#vulnerabilities","title":"Vulnerabilities","text":"<p>If an observation is a vulnerability with a CVE or GHSA number, the Vulnerability ID in the <code>Vulnerability</code> box will be a link to the National Vulnerabilities Database (NVD) or the GitHub Advisory Database.</p> <p></p>"},{"location":"integrations/links/#components","title":"Components","text":"<p>If an observation has a component with a PURL as its origin and the package type is in</p> <ul> <li>Cargo</li> <li>Go</li> <li>Maven</li> <li>npm</li> <li>NuGet</li> <li>PyPI</li> </ul> <p>the Component PURL in the <code>Origins</code> box will be a link to the open/source/insights platform.</p> <p></p> <p>open/source/insights (https://deps.dev) provides insights into the open source component containing the vulnerability. It helps you to understand the security, licensing, and maintenance aspects of the component.</p> <p></p>"},{"location":"integrations/notifications/","title":"Notifications","text":"<p>SecObserve can send notifications to email addresses, Microsoft Teams or Slack for 3 kinds of events:</p> <ul> <li>When the security gate of a product changes.</li> <li>When an exception occurs while processing a request.</li> <li>When an exception occurs in a background task.</li> </ul> <p>There is a ratelimiting active to prevent flooding of notifications, if a series of exceptions occurs. The same exception is sent only once during a specified timedelta, which can be configured in the Django Admin user interface. The default for this timedelta is 1 hour.</p>"},{"location":"integrations/notifications/#notifications-to-email-addresses","title":"Notifications to email addresses","text":""},{"location":"integrations/notifications/#settings-in-secobserve","title":"Settings in SecObserve","text":"<p>The field <code>EMAIL_FROM</code> needs to be set in the Django Admin user interface to be able to send notifications to email addresses for both events. </p>"},{"location":"integrations/notifications/#notifications-for-security-gates","title":"Notifications for security gates","text":"<p>When creating or editing a product, the field <code>Email</code> can be set in the Notification section with a comma separated list of email addresses. If the security gate of the product changes and this field is filled, then a notification is sent each of the email addresses.</p> <p></p>"},{"location":"integrations/notifications/#notifications-for-exceptions","title":"Notifications for exceptions","text":"<p>An admistrator can configure the field <code>EXCEPTION_EMAIL_TO</code> in the Django Admin user interface. If an exception occurs while processing a request and this field is filled with a comma separated list of email addresses, a notifications is sent each of the email addresses before returning the HTTP code 500 via the REST API.</p>"},{"location":"integrations/notifications/#notifications-to-microsoft-teams-and-slack","title":"Notifications to Microsoft Teams and Slack","text":""},{"location":"integrations/notifications/#settings-in-microsoft-teams","title":"Settings in Microsoft Teams","text":"<p>For both types of notifications an incoming webhook has to be set for a channel, where the notifications shall appear. How to do this is explained in Create Incoming Webhooks. Copy the URL of the webhook to the clipboard, to have it available to set it in SecObserve.</p> <p>The messages do not include mentions, but a user can set the \"Channel notifications\" to \"All activities\" in Teams, to get an active notification when an entry is generated. </p>"},{"location":"integrations/notifications/#settings-in-slack","title":"Settings in Slack","text":"<p>For both types of notifications an incoming webhook has to be set for a channel, where the notifications shall appear. How to do this is explained in Sending messages using Incoming Webhooks. Copy the URL of the webhook to the clipboard, to have it available to set it in SecObserve.</p>"},{"location":"integrations/notifications/#notifications-for-security-gates_1","title":"Notifications for security gates","text":"<p>When creating or editing a product, the fields <code>MS Teams</code> and/or <code>Slack</code> can be set in the Notification section with the copied webhook URL. If the security gate of the product changes and this field is filled, then a notification is sent to Microsoft Teams and/or Slack.</p> <p></p>"},{"location":"integrations/notifications/#notifications-for-exceptions_1","title":"Notifications for exceptions","text":"<p>An admistrator can configure the fields <code>EXCEPTION_MS_TEAMS_WEBHOOK</code> and/or <code>EXCEPTION_SLACK_WEBHOOK</code> in the Django Admin user interface. If an exception occurs while processing a request and this field is filled with the copied webhook URL, a notifications is sent to Microsoft Teams and/or Slack before returning the HTTP code 500 via the REST API.</p>"},{"location":"integrations/notifications/#notifications-in-the-user-interface","title":"Notifications in the user interface","text":"<p>Notifications are also stored in the database and can be viewed in the user interface.</p> <ul> <li>Regular users can view notifications for changed security gates and exceptions in background tasks for all products where they are a product member.</li> <li>Administrators can view all notifications.</li> </ul> <p></p> <p>When a notification is deleted, it is removed from the database and won't be visible anymore for all users.</p>"},{"location":"integrations/observations_export/","title":"Export of observations","text":"<p>Observations of a product can be exported to Excel or CSV. When showing a product, there is an <code>Export</code> button. When clicking it, it shows a menu with several options to export the observations of this product:</p> <p></p>"},{"location":"integrations/oidc_authentication/","title":"OpenID Connect authentication","text":"<p>OpenID Connect authentication has been tested with Keycloak and Azure Active Directory. It should work with other OpenID Connect providers as well, as long as they support the authorization flow with PKCE and without a secret.</p>"},{"location":"integrations/oidc_authentication/#keycloak","title":"Keycloak","text":"<p>In Keycloak a new OpenID Connect client needs to be created. The client needs to be configured as follows, assuming the frontend is available at <code>https://secobserve.example.com</code>:</p> <p></p>"},{"location":"integrations/oidc_authentication/#configuration-parameters-for-secobserve","title":"Configuration parameters for SecObserve","text":"<p>Backend</p> Environment variable Value <code>OIDC_AUTHORITY</code> <code>https://keycloak.example.com/realms/NAME_OF_REALM</code> <code>OIDC_CLIENT_ID</code> <code>CLIENT_ID</code> <code>OIDC_USERNAME</code> <code>preferred_username</code> <code>OIDC_FIRST_NAME</code> <code>given_name</code> <code>OIDC_LAST_NAME</code> <code>family_name</code> <code>OIDC_EMAIL</code> <code>email</code> <p>Frontend</p> Environment variable Value <code>OIDC_ENABLE</code> <code>true</code> <code>OIDC_AUTHORITY</code> <code>https://keycloak.example.com/realms/NAME_OF_REALM</code> <code>OIDC_CLIENT_ID</code> <code>CLIENT_ID</code> <code>OIDC_REDIRECT_URI</code> <code>https://secobserve.example.com</code> <code>OIDC_POST_LOGOUT_REDIRECT_URI</code> <code>https://secobserve.example.com</code>"},{"location":"integrations/oidc_authentication/#azure-active-directory","title":"Azure Active Directory","text":"<p>In Azure Active Directory a new App registration needs to be created.</p>"},{"location":"integrations/oidc_authentication/#configuration-parameters-for-secobserve_1","title":"Configuration parameters for SecObserve","text":"<p>Backend</p> Environment variable Value <code>OIDC_AUTHORITY</code> <code>https://login.microsoftonline.com/TENANT_ID/v2.0</code> <code>OIDC_CLIENT_ID</code> <code>CLIENT_ID</code> <code>OIDC_USERNAME</code> <code>preferred_username</code> <code>OIDC_FULL_NAME</code> <code>name</code> <code>OIDC_EMAIL</code> <code>email</code> <p>Frontend</p> Environment variable Value <code>OIDC_ENABLE</code> <code>true</code> <code>OIDC_AUTHORITY</code> <code>https://login.microsoftonline.com/TENANT_ID</code> <code>OIDC_CLIENT_ID</code> <code>CLIENT_ID</code> <code>OIDC_REDIRECT_URI</code> <code>https://secobserve.example.com</code> <code>OIDC_POST_LOGOUT_REDIRECT_URI</code> <code>https://secobserve.example.com</code>"},{"location":"integrations/overview/","title":"Overview","text":""},{"location":"integrations/rest_api/","title":"REST API","text":"<p>SecObserve is build with an API first approach, every functionality needed to use SecObserve is covered by the REST API.</p>"},{"location":"integrations/rest_api/#authentication","title":"Authentication","text":""},{"location":"integrations/rest_api/#jwt","title":"JWT","text":"<p>JWT authentication is used by SecObserve's frontend.</p> Endpoint <code>/api/authentication/authenticate/</code> Validity duration for regular users 7 days / 168 hours  <sup>1)</sup> Validity duration for superusers 1 day / 24 hours  <sup>1)</sup> HTTP header <code>Authorization: JWT</code><code>token</code> <p><sup>1)</sup> Values can be changed by the administrators.</p>"},{"location":"integrations/rest_api/#api-token","title":"API token","text":"<p>API tokens are used for other integration scenarios, e.g. to call the REST API from a CI/CD pipeline to import observations.</p> Validity Until revokation HTTP header <code>Authorization: APIToken</code><code>token</code> <p>API tokens can be created for a product or a user.</p>"},{"location":"integrations/rest_api/#product-api-token","title":"Product API token","text":"<p>A role (see Roles and permissions) must be selected during creation of a product API token, to determine the permissions of the API token for the product.</p> <p></p> <p>The API token can be seen only once after it has been created. It must be copied to ensure that it is not lost.</p> <p></p> <p>Only one API token can be created per product. If it needs to be replaced, it must be revoked first.</p> <p></p>"},{"location":"integrations/rest_api/#user-api-token","title":"User API token","text":"<p>An API token for a user can only be created and revoked with API calls. The token can be seen only once, when it is created. Afterwards there is no way to see that API token again. If it is lost it needs to be revoked and a new one has to be created, as only one API token is allowed per user.</p> <p>The API token has the same permissions for the same products as the user.</p> Endpoint to create API token <code>/api/authentication/create_api_token/</code> Endpoint to revoke API token <code>/api/authentication/revoke_api_token/</code>"},{"location":"integrations/rest_api/#interactive-api-documentation","title":"Interactive API documentation","text":"<p>The full documentation of the REST API is available at <code>&lt;BACKEND_URL&gt;/api/oa3/swagger-ui</code>.</p>"},{"location":"integrations/source_code_repositories/","title":"Source code repositories","text":"<p>Observations can have a source file plus start and end lines as an origin. During the assessment of the observation it is often helpful to view the source code.</p>"},{"location":"integrations/source_code_repositories/#setting-the-repository-in-a-product","title":"Setting the repository in a product","text":"<p>When creating or editing a product, the field <code>Repository prefix</code> can be set. This needs to be the prefix of the URL to show a file in the repository. </p> <ul> <li>If the observations of the product have the branch set, then for GitLab it is something like <code>https://gitlab.maibornwolff.de/secobserve/secobserve/-/blob</code>, for GitHub it looks like <code>https://github.com/MaibornWolff/codecharta/blob</code>. </li> <li>If the observations don't have the branch set, then a branch need to be at the end of the repository prefix, e.g. <code>https://gitlab.maibornwolff.de/secobserve/secobserve/-/blob/dev</code> or <code>https://github.com/MaibornWolff/codecharta/blob/dev</code>.</li> <li>Azure DevOps does not need the branch in the repository prefix, an example is <code>https://dev.azure.com/maibornwolff/SecObserve/_git/SecObserve_Frontend</code>. If the observations have a branch set, then this branch will be used in the URL, otherwise the default branch of the repository will be used.</li> </ul> <p></p>"},{"location":"integrations/source_code_repositories/#showing-the-link","title":"Showing the link","text":"<p>If the <code>Repository prefix</code> is set in the product and the observation has a source file as an origin, then name of the source file will be shown as a link to the source in the repository.</p> <p></p>"},{"location":"usage/assess_observations/","title":"Assess observations","text":"<p>With an assessment of an observation the user can change two attributes of an observation:</p> <ul> <li>The severity given by the parser must not necessarily match the severity of the observation for the current product.</li> <li>All observations have initially the status <code>Open</code>. The result of an investigation how to deal with the observation might say, the observation must not be fixed because it is ...<ul> <li>... <code>In review</code> and needs further investigation.</li> <li>... already <code>Resolved</code>. You have to be aware that the observation will be set back to <code>Open</code> if it will be found in a subsequent import.</li> <li>... a <code>Duplicate</code> of another observation.</li> <li>... a <code>False positive</code> that has been detected by the scanner wrongly.</li> <li>... <code>Risk accepted</code>, a decision that a breach because of that observation can be managed.</li> <li>The system is <code>Not affected</code> because the observation has been mitigated by a measure.</li> </ul> </li> </ul> <p>The dialog to enter the assessment can be opened when showing the observation: </p> <p></p> <p>In the assessment dialog the user can change either the severity and/or the status and has to enter a mandatory comment to explain the change:</p> <p></p> <p>A new entry with the changed values is stored in the <code>Observation Log</code> after the assessment has been saved.</p>"},{"location":"usage/branches/","title":"Branches","text":"<p>Branches are an optional feature of the product. They can be used to separate the observations for different branches of a product. If only one branch is used to develop a product, the branch feature can be ignored.</p>"},{"location":"usage/branches/#list-of-branches","title":"List of branches","text":"<p>A product has a list of branches. They can either be created manually from the Branches tab of the product or will be created automatically, when observations are imported using a branch name that didn't exist before for that product.</p> <p></p> <p>The list of branches shows the severities of open observations for each branch.</p> <p>Clicking on the name of a branch brings up the list of open observations for that branch.</p> <p>Warning</p> <p>When a branch is deleted, all observations for that branch will be deleted as well.</p>"},{"location":"usage/branches/#repository-default-branch","title":"Repository default branch","text":"<p>The Repository default branch should always be set, when branches are used in the observations.</p> <ul> <li>The metrics on the dashboard and on the Metrics tab are calculated using the observations where the default branch is set.</li> <li>The number of severities in the header when showing a product are for the observations where the default branch is set as well.</li> <li>Issues in GitHub, GitLab or Jira are created only for the default branch.</li> <li>The default branch cannot be deleted and is exempt from the housekeeping.</li> </ul> <p>The repository default branch can be set manually while editing a product. If it is not set manually, it will be set automatically with the first branch that is created, either after importing observations with a branch name or by manually creating a branch.</p> <p></p> <p>The Observations tab shows a button to show all open observations for the default branch.</p> <p></p>"},{"location":"usage/branches/#housekeeping","title":"Housekeeping","text":"<p>Inactive branches will be deleted automatically after a certain time. Inactivity is defined as the number of days since the last import of observations for a branch.</p>"},{"location":"usage/branches/#parameters","title":"Parameters","text":"<p>The parameters are set globally in the admin user interface and can be partially overridden per product.</p> Parameter global Description BRANCH_HOUSEKEEPING_CRONTAB_MINUTES Minutes crontab expression for branch housekeeping BRANCH_HOUSEKEEPING_CRONTAB_HOURS Hours crontab expression for branch housekeeping (UTC) BRANCH_HOUSEKEEPING_ACTIVE If this parameter is set, inactive branches will be deleted automatically. BRANCH_HOUSEKEEPING_KEEP_INACTIVE_DAYS Days before incative branches and their observations are deleted BRANCH_HOUSEKEEPING_EXEMPT_BRANCHES Regular expression which branches to exempt from deletion <p>Per default the task to delete inactive branches including their observations is scheduled to run every night at 02:00 UTC time. This default can be changed by administrators via the admin user interface. The expressions for <code>BRANCH_HOUSEKEEPING_CRONTAB_MINUTES</code> and <code>BRANCH_HOUSEKEEPING_CRONTAB_HOURS</code> have to be valid values according to https://huey.readthedocs.io/en/latest/api.html#crontab:</p> <ul> <li><code>*</code> = every distinct value (every minute, every hour)</li> <li><code>*/n</code> = run every <code>n</code> times, i.e. hours=\u2019*/4\u2019 == 0, 4, 8, 12, 16, 20</li> <li><code>n</code> = run every <code>n</code> (minutes 0 - 60, hours 0 - 24)</li> <li><code>m-n</code> = run every time m..n</li> <li><code>m,n</code> = run on m and n</li> </ul> <p>Hours are always in UTC time.</p>"},{"location":"usage/branches/#product-specific-settings","title":"Product specific settings","text":"<p>A product can override the housekeeping behaviour by setting the <code>Housekeeping</code> attribute:</p> <ul> <li>Standard: Use the instance-wide definition, this is the default.</li> <li>Disabled: Do not delete inactive branches for that product.</li> <li>Product specific: Use product specific settings for deletion of inactive branches.</li> </ul> <p></p>"},{"location":"usage/branches/#protect-branches","title":"Protect branches","text":"<p>A branch can be proceted to prevent it from being deleted by the housekeeping task. This can be done by setting the <code>Protect from housekeeping</code> attribute of a branch.</p>"},{"location":"usage/duplicates/","title":"Duplicates","text":"<p>Duplicate observations can appear when different scanners are used to scan the same code or dependencies. Sometimes one scanner might produce potential duplicates as well, e.g. reporting the same vulnerability for multiple dependencies.</p>"},{"location":"usage/duplicates/#identify-potential-duplicates","title":"Identify potential duplicates","text":"<p>An asynchronous process to detect potential duplicates is executed when importing scan results. It checks all existing open observations for the same project, the same branch and the same service plus one of two conditions:</p> <ul> <li>The same title for different components</li> <li>Observations with the same source file name and start line number from different scanners</li> </ul> <p>The list of observations in the user interface shows if an observation has potential duplicates.</p>"},{"location":"usage/duplicates/#mark-duplicates","title":"Mark duplicates","text":"<p>If an observation has potential duplicates, they are shown in a list when showing the observation details. The user can tick one or more of them and mark them as duplicates. This will add an assessment to the observation with the status <code>DUPLICATE</code> and a comment.</p>"},{"location":"usage/import_observations/","title":"Import observations","text":""},{"location":"usage/import_observations/#import-from-ci-pipelines-via-the-api","title":"Import from CI pipelines via the API","text":"<p>GitHub actions and GitLab CI templates support running vulnerability checks and importing the results into SecObserve via GitHub workflows or GitLab CI pipelines in an efficient way.</p>"},{"location":"usage/import_observations/#import-from-the-frontend","title":"Import from the frontend","text":"<p>Alternatively observations can be imported via the user interface. When showing a product, there are buttons to either upload a file or to import from an API:</p> <p></p>"},{"location":"usage/import_observations/#upload-of-files","title":"Upload of files","text":"<p>A file and a respective parser for the file format need to be selected. Optional are attributes for the branch, the origin as service, docker image and endpoint URL.</p>"},{"location":"usage/import_observations/#api-import","title":"API import","text":"<p>Before importing observations from an API, an API configuration needs to be created for the product. This API configuration specifies how to access the API (URL, API key, ...). Optional for the import are attributes for the branch, the origin as service, docker image and endpoint URL.</p>"},{"location":"usage/import_observations/#import-algorithm","title":"Import algorithm","text":"<p>The import algorithm has to decide, if an observation already exists and needs to be updated or it is new and needs to be created. But how does the import algorithm identifies an observation to make this decision? Two terms help to understand how that works:</p> <ul> <li>Identity hash: The <code>identity hash</code> is a SHA256 hash code of the concatenation of the observation's title and all its origins <sup>[1]</sup>. Two observations with the same <code>identity hash</code> are defined as identical.</li> <li>Vulnerability check: An import for one product, one branch and one file name resp. one API configuration is a so-called vulnerability check.</li> </ul> <p>A flowchart visualizes the import algorithm:</p> <pre><code>    flowchart TD\n    read(Read observations for a vulnerability check) --&gt; all_observations{For all observations\\nof this vulnerability check}\n    all_observations -- with observation --&gt; identity{Identical observation\\nin the same\\nvulnerability check?}\n    subgraph Create or update\n    identity -- yes --&gt; update(Update existing observation)\n    identity -- no --&gt; create(Create new observation)\n    end\n    all_observations -- finished ----&gt; resolved(Set status to `Resolved` for all untouched observations of this vulnerability check)\n</code></pre> <p>[1]: The tag of the docker image is not part of the <code>identity hash</code> to allow updates of the docker image without creating a new observation.</p>"},{"location":"usage/metrics/","title":"Metrics","text":"<p>Metrics about observations are shown in 2 places:</p> <ul> <li>On the dashboard a user can see the aggregated metrics of the observations for all products they have access to.</li> <li>On the Metrics tab of a product a user can see the metrics for the observations of that product.</li> </ul> <p></p> <p>Metrics are calculated in an asychronous task. Per default the task is scheduled to run every 5 minutes. This default can be changed by administrators via the admin user interface.</p>"},{"location":"usage/product_groups/","title":"Product groups","text":"<p>A product group is a collection of products. It is used to group products that belong together, e.g. because they are part of the same project.</p> <p>Users cannot see only the list of the associated products and their combined metrics, but there are some settings that are shared between all products of the group:</p> <ul> <li>Rules defined for a product group are applied to all products in the group in addition to the rules defined for the product.</li> <li>Members defined for a product group have access to the products in the group in addition to the members defined for the product.</li> <li>The API token of a product group can be used to access the API for all products in the group.</li> <li>Housekeeping for branches:<ul> <li>Standard: The branches of all products in the group are deleted according to the settings of the product.</li> <li>Disabled: Housekeeping for branches is disabled for all products in the group.</li> <li>Product group specific: The branches of all products in the group are deleted according to the settings of the product group.</li> </ul> </li> <li>The settings for Notifications are used, if no notification settings are defined for the product. If there are notification settings defined for the product, they override the settings of the product group.</li> <li>Security gates:<ul> <li>Standard: The security gates of all products in the group are calculated according to the settings of the product.</li> <li>Disabled: Security gates are disabled for all products in the group.</li> <li>Product group specific: Security gates of all products in the group are calculated according to the settings of the product group.</li> </ul> </li> </ul>"},{"location":"usage/rule_engine/","title":"Rule engine","text":"<p>Sometimes the result of a scanner doesn't fit to the product's needs. Either the severity or the status need to be adjusted. To avoid having to do many manual assessments regularly, a built-in rule engine can adjust severity and/or status directly with the import of observations.</p> <p>This can remove a lot of noise, for example by setting observations to <code>False positive</code>, in case the ruleset of the scanner can not be adjusted appropriately. </p> <p></p> <p>Rules can be managed in two ways:</p> <ul> <li>General rules will be applied for all products. A product can be excluded from general rules in its settings.</li> <li>Product Rules are only valid for one product.</li> </ul> <p>These fields are used to decide if a rule shall be applied for an observation:</p> <ul> <li>Parser (mandatory): The observation has been imported with this parser.</li> <li>Scanner prefix (optional): The observation has been generated by a scanner which name starts with this prefix. A prefix is used here because the scanner field in the observation often contains the version of the scanner as well, which is typically irrelevant for the rule.</li> <li>Observation title (optional): Regular expression to match the observation's title</li> <li>Origin component name:version (optional): Regular expression to match the component name:version</li> <li>Origin docker image name:tag (optional): Regular expression to match the docker image name:tag</li> <li>Origin endpoint URL (optional): Regular expression to match the endpoint URL</li> <li>Origin service name (optional): Regular expression to match the service name</li> <li>Origin source file (optional): Regular expression to match the source file</li> <li>Origin cloud qualified resource (optional): Regular expression to match the cloud qualified resource, which is the concatenation of account (AWS) or subscription (Azure) or project (GCP) with the resource</li> </ul> <p>If an observation matches all fields containing a value, than the new severity and/or new status is set in the observation and a comment is stored in the <code>Observation Log</code>.</p>"},{"location":"usage/security_gates/","title":"Security gates","text":"<p>A security gate shows, that a product does not exceed a defined amount of vulnerabilities per severity. It can be <code>Passed</code> if the product is under or at the defined thresholds or <code>Failed</code> if the product has more observations for at least one severity.</p> <p></p> <p>There is an instance-wide definition of the thresholds, that can be changed by an administrator. The default is:</p> Severity Threshold Critical 0 High 0 Medium 99999 Low 99999 None 99999 Unkown 99999 <p>A product can decide how to deal with security gates by setting the <code>Security gate</code> attribute:</p> <ul> <li>Standard: Use the instance-wide definition, this is the default.</li> <li>Disabled: Do not calculate and show a security gate.</li> <li>Product specific: Use product specific thresholds to calculate the security gate.</li> </ul> <p></p>"},{"location":"usage/supported_scanners/","title":"Supported scanners","text":""},{"location":"usage/supported_scanners/#types","title":"Types","text":"<p>There are different types of vulnerability scans:</p> <ul> <li>SCA / Software Composition Analysis: Modern systems are not completely rewritten from scratch, but many basic functions are used as libraries. This applies not only to application code, but in the case of Docker, also to operating system functions and programmes. All these components can have known vulnerabilities that can be exploited by attackers.</li> <li>Application SAST / Static Application Security Testing: Many problems can be detected in the code through rule-based searches, e.g. injections or weak encryption. Tools exist for all common programming languages.</li> <li>Infrastructure SAST: Also for Infrastructure as Code (Dockerfile, Helm Charts, Terraform, ...) many problems can be found with rule-based searches before applying the code to set up the infrastructure.</li> <li>Secrets: Secrets such as passwords or API keys must not be checked into repositories with the code, and there are tools that search, for example, Git repositories across the entire version history for such secrets.</li> <li>DAST / Dynamic Application Security Testing:: This class is black-box security testing where the tests are performed by attacking an application (typically web applications or APIs) from the outside. The tests can be passive, where only anomalies are looked for, or active attacks on the system. </li> <li>Cloud infrastructure: The running infrastructure, e.g. a Kubernetes cluster, can also be checked for vulnerabilities both with internal views (tests that run inside the infrastructure) and external views (tests from outside via the internet).</li> <li>IAST / Interactive Application Security Testing: IAST works inside an application by instrumenting the code to detect and report issues while the application is running.</li> </ul>"},{"location":"usage/supported_scanners/#data-formats","title":"Data formats","text":"<p>While every vulnerability scanner writes its own format, there are 2 standardized formats that are implemented by several scanners:</p> <ul> <li>CycloneDX: CycloneDX is a Software Bill of Material (SBOM), that contains information about components of a system and their vulnerabilities. It is typically used by SCA scanners.</li> <li>SARIF: The Static Analysis Results Interchange Format is an OASIS standard which is implemented by a lot of SAST scanners.</li> </ul> <p>This means the <code>CycloneDX</code> and <code>SARIF</code> parsers can import data from a variety of vulnerability scanners, while other vulnerability scanners need a dedicated parser for their special data format.</p>"},{"location":"usage/supported_scanners/#scanners","title":"Scanners","text":"<p>These scanners have been tested with SecObserve:</p> Scanner Parser Type Source Dependency Check SARIF <sup>1)</sup> SCA API Dependency Track Dependency Track SCA API Grype CycloneDX SCA File Trivy CycloneDX SCA File Bandit SARIF Application SAST File ESLint SARIF Application SAST File Find-Sec-Bugs SARIF Application   SAST File Semgrep SARIF Application SAST File Checkov SARIF Infrastructure SAST File KICS SARIF Infrastructure SAST File tfsec SARIF Infrastructure SAST File Trivy SARIF Infrastructure SAST File Gitleaks SARIF Secrets File Trivy SARIF Secrets File CryptoLyzer <sup>2)</sup> CryptoLyzer DAST File DrHeader DrHeader DAST File ZAP ZAP DAST File Azure Defender for Cloud <sup>3)</sup> Azure Defender Cloud infrastructure File Prowler Prowler Cloud infrastructure File <p><sup>1)</sup> This is the exception to the rule. Even though SARIF is more suited for static code analysis, it works for Dependency Check.</p> <p><sup>2)</sup> The CryptoLyzer parser checks the results (TLS versions, cipher suites, elliptic curves and signature algorithms) against BSI (Bundesamt f\u00fcr Sicherheit in der Informationssicherheit) recommendations.</p> <p><sup>3)</sup> The results of Azure Defender for Cloud have to be exported manually in CSV format from the Azure Portal.</p> <p>GitHub actions and GitLab CI templates support running vulnerability checks and importing the results into SecObserve via GitHub workflows or GitLab CI pipelines in an efficient way.</p>"},{"location":"usage/users_permissions/","title":"Users and permissions","text":""},{"location":"usage/users_permissions/#users","title":"Users","text":"<p>SecObserve supports two types of users:</p> <ul> <li>Internally managed users: You need a username and password given by a SecObserve administrator and use the <code>SIGN IN WITH USER</code> button.</li> <li>Users managed in a directory: The button <code>ENTERPRISE SIGN</code> will redirect you to login page of your users directory, if OpenID Connect is configured.</li> </ul> <p>Users have specified permissions depending on their type and role in a product.</p>"},{"location":"usage/users_permissions/#user-types","title":"User types","text":"<p>The user type can be set by flags in the user administration:</p> <ul> <li>Superusers are the administrators of the system.</li> <li>External users do not belong to the organization, e.g. customers or partners.</li> <li>Internal users are all users who are not superusers or external users.</li> </ul> <p>There are some general permissions based on the user's type:</p> Internal External Superuser Create Product Groups X - X Create Product X - X View General Rules X X X Create General Rules - - X Edit General Rules - - X Delete General Rules - - X Access Admin UI - - X"},{"location":"usage/users_permissions/#roles-and-permissions","title":"Roles and permissions","text":"<p>While superusers have permission to view and edit all data, internal and external users must be a product member with a specific role to access the product and its data. Product members of a product group have access to all products of that group with their respective role.</p> Reader Writer Maintainer Owner Upload View Product Group X X X X - Edit Product Group - - X X - Delete Product Group - - - X - View Product X X X X - Import Observations - X X X X Edit Product - - X X - Delete Product - - - X - View Observation X X X X - Create Observation - X X X - Edit Observation <sup>1)</sup> - X X X - Assess Observation - X X X - Delete Observation - - - X - View Product Rules X X X X - Create Product Rules - - X X - Edit Product Rules - - X X - Apply Rules to Product - - X X - Delete Product Rules - - X X - View API Configuration X X X X - Create API Configuration - - X X - Edit API Configuration - - X X - Delete API Configuration - - X X - View Product Member X X X X - Create Product Member - - X <sup>2)</sup> X - Edit Product Member - - X <sup>2)</sup> X - Delete Product Member - - X <sup>2)</sup> X - <p><sup>1)</sup> Only manually created observations can be edited</p> <p><sup>2)</sup> Maintainers are not allowed to manipulate Owners of that product</p>"}]}